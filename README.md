# On the Compositional Skills of Sequence-to-Sequence Transformers

This repository implements a standard encoder-decoder transformer model for
the SCAN task, as put forth in [Brendan Lake & Marco Beroni, 2018](http://proceedings.mlr.press/v80/lake18a/lake18a.pdf)

# Results

As in the original paper, all models were trained for approximately 100 000
steps, where each optimization step sees an input/output example. Below are
som results, model were tuned with an  informal hyperparameter search.
